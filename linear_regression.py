# -*- coding: utf-8 -*-
"""Linear_Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Nf6hTvdWdeIhLqlkwq-2AQo2mNcr68r
"""

#CHANGE THE PATH TOO IF U OPENED THIS DOC
from google.colab import drive
drive.mount('/content/drive')

"""# **Step 1:** AIM - Predicting the Closing Price of the Stock.

In this project, Machine Learning is used to predict the closing price of a stock based on historical market data. ML models learn the underlying patterns in features like Open, High, and Low, and use them to estimate the Close value.
"""

# importing the necessary modules and packages
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline

"""# **Step 2:** Data Loading"""

# loading the data
df = pd.read_csv(r'/content/drive/MyDrive/apple_stock.csv')
df

"""# **Step 3:** Data Information Gathering"""

#finding the number of rows and columns
df.shape

# display the information related to dataset
df.info()

# to generate descriptive statistics
df.describe()

#display the first n number of rows
df.head()

#display the n number of rows from bottom
df.tail(10)

# counts the number of non-null values in each column (by default)
df.count()

#count the total number of duplicate rows
df.duplicated().sum()

#count the number of missing (NaN) values in each column
df.isnull().sum()

"""# **Step 4:** Data Cleaning"""

#dropping columns which are not required
df.drop(['Date', 'Adj Close', 'Volume'], axis=1, inplace=True)
df

"""# **Step 5:** Data Visualization"""

#line plot of close price
plt.plot(df['Close'])
plt.xlabel('Days')
plt.ylabel('Close Price($)')
plt.title('Close Price Over Time')
plt.show()

#histogram of close
plt.hist(df['Close'])
plt.xlabel('Close Price($)')
plt.ylabel('Frequency')
plt.title('Close Price Distribution')
plt.show()

#scatter plot
plt.scatter(df['Open'], df['Close'])
plt.xlabel('Opening price($)')
plt.ylabel('Close Price($)')
plt.title('Opening Price vs Closing Price')
plt.show()

#boxplot
sns.boxplot(data=df, x='Open')
plt.title('Opening price vs closing price')
plt.show()

#regression plot
sns.regplot(data=df, x='Open', y='Close')
plt.xlabel('Opening price($)')
plt.ylabel('Close Price($)')
plt.title('Opening Price vs Close Price')

"""# **Step 6:** Finding Correlation (Heatmap)"""

#IQR calculating
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
IQR

#calculate percentage of outliers using IQR


# Calculate Q1, Q3, and IQR
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1

# Calculate outliers using IQR rule
outlier_condition = ((df < (Q1 - 1.5 * IQR)) |
                     (df > (Q3 + 1.5 * IQR)))

# Count outliers per column
outlier_counts = outlier_condition.sum()

# Total rows in dataset
total_rows = len(df)

# Calculate % of outliers
outlier_percent = (outlier_counts / total_rows) * 100

# Show results
print("Outliers per column (%):\n")
print(outlier_percent)

#Z-score
a = df.mean()
b = df.std()
z = (df - a)/b
print(z)

#heatmap

sns.heatmap(df.corr(), annot=True)

#Correlation between Volume and Close = -0.19
#This is weak and even negative, so it’s not strongly helpful.

"""# **Step 7:** Dividing data into X and Y

Input (Features): Open, High, Low

Output (Target): Close (numerical continuous value)
"""

# dividing into x(input column)
X = df[['Open', 'High', 'Low']]
# Y(target column)
Y = df['Close']


X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

X_test

Y_test

"""# **Step 8:** Applying Algorithms

| Algorithm               | Description                                                        |
| ---------------------  | ------------------------------------------------------------------ |
| **Linear Regression**    | Fits a linear relationship between inputs and target               |
| **Decision Tree**      | Splits data into decision nodes using thresholds                   |
| **Random Forest**          | Combines multiple decision trees to improve accuracy               |
| **SVR (SVM)**          | Maps features into high-dimensional space to fit complex functions |

"""

#sample inputs

sample_input1 = np.array([[0.333705,	0.347098,	0.332589]])
sample_input2 = np.array([[0.138951,	0.142857,	0.138951]])
sample_input3 = np.array([[0.127232,	0.130580,	0.123884]])

# linear regression

regression = LinearRegression()
regression.fit(X_train, Y_train)
y_pred1 = regression.predict(X_test)
score1 = r2_score(Y_test, y_pred1)

print("Accuracy:", score1)

#sample input
pred1_1 = regression.predict(sample_input1)
print("Predicted outcome:", pred1_1)

pred1_2 = regression.predict(sample_input2)
print("Predicted outcome:", pred1_2)

pred1_3 = regression.predict(sample_input3)
print("Predicted outcome:", pred1_3)

regressor = DecisionTreeRegressor(max_depth=4, random_state=42)  # Limiting depth for better visualization
regressor.fit(X, Y)

# Plot the tree
plt.figure(figsize=(20, 10))
plot_tree(regressor, filled=True, feature_names=X.columns, class_names=['Close'], rounded=True, proportion=True, precision=2)
plt.title("Decision Tree Regressor for Stock Closing Price")
plt.show()

#decision Tree

from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import  r2_score

regression1 = DecisionTreeRegressor()
regression1.fit(X_train, Y_train)
y_pred2 = regression1.predict(X_test)
score2 = r2_score(Y_test,y_pred2)
print("Accuracy:", score2)

#sample input
pred2_1 = regression1.predict(sample_input1)
print("Predicted outcome:", pred2_1)

pred2_2 = regression1.predict(sample_input2)
print("Predicted outcome:", pred2_2)

pred2_3 = regression1.predict(sample_input3)
print("Predicted outcome:", pred2_3)

# Random Forest with Hyperparameter Tuning

rf_params = {
    'n_estimators': [50, 100],
    'max_depth': [4, 6, None],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

regression2 = GridSearchCV(RandomForestRegressor(random_state=42), rf_params, cv=5, scoring='r2', n_jobs=-1)
regression2.fit(X_train, Y_train)
y_pred3 = regression2.predict(X_test)
score3 = r2_score(Y_test, y_pred3)
print("Accuracy:", score3)

#sample input
pred3_1 = regression2.predict(sample_input1)
print("Predicted outcome:", pred3_1)

pred3_2 = regression2.predict(sample_input2)
print("Predicted outcome:", pred3_2)

pred3_3 = regression2.predict(sample_input3)
print("Predicted outcome:", pred3_3)

#SVM (with SCALING)

from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler

# standard scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

regression3 = SVR()
regression3.fit(X_train_scaled, Y_train)
y_pred4 = regression3.predict(X_test_scaled)
score4 = r2_score(Y_test, y_pred4)
print("Accuracy:", score4)

# sample input
pred4_1 = regression3.predict(scaler.transform(sample_input1))
print("Predicted outcome:", pred4_1)

pred4_2 = regression3.predict(scaler.transform(sample_input2))
print("Predicted outcome:", pred4_2)

pred4_3 = regression3.predict(scaler.transform(sample_input3))
print("Predicted outcome:", pred4_3)

"""# **Step 9:**  Model Evaluation
We evaluated model performance using:

R² Score: Indicates how well the model explains the variance in the data.
Closer to 1.0 = better predictive power.

Each model was also tested on sample inputs to compare real-world prediction behavior.


"""

a = pd.DataFrame({'Algorithm': ['Linear Regression', 'Decision Regressor', 'Random Forest', 'SVM'], 'Accuracy': [score1, score2, score3, score4], 'Input 1': [sample_input1,sample_input1, sample_input1,sample_input1], 'Actual': [0.343192, 0.343192,0.343192,0.343192] ,'Prediction': [pred1_1, pred2_1, pred3_1, pred4_1]})
a

b = pd.DataFrame({'Algorithm': ['Linear Regression', 'Decision Regressor', 'Random Forest', 'SVM'], 'Accuracy': [score1, score2, score3, score4], 'Input 2': [sample_input2, sample_input2, sample_input2, sample_input2], 'Actual': [	0.141183,	0.141183,	0.141183,	0.141183] ,'Prediction': [pred1_2, pred2_2,pred3_2,pred4_2]})
b

c = pd.DataFrame({'Algorithm': ['Linear Regression', 'Decision Regressor', 'Random Forest', 'SVM'], 'Accuracy': [score1, score2, score3, score4],'Input 3': [sample_input3, sample_input3, sample_input3,sample_input3], 'Actual': [	0.125000,0.125000,0.125000,0.125000] ,'Prediction': [pred1_3, pred2_3,pred3_3,pred4_3]})
c

"""# **Conclusion**
Linear Regression, Decision Tree, and Random Forest achieved near-perfect R² scores on test data.

SVM improved significantly after applying feature scaling.
"""